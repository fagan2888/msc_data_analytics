load("/Users/MariaAthena/Dropbox/00 Imperial College/Statistics and Econometrics/Lecture 2/wage1.RData")
### Maria Athena B. Engesaeth
### CID 01159179
##################################################################################################
setwd("/Users/MariaAthena/Dropbox/00 Imperial College/Statistics and Econometrics/Problem Set 2")
##################################################################################################
## 1. Data on prices at fast-food resturant by zip code in the US.
load(file="discrim.RData")
desc
# A. Model examining the price of soda in its log form, in terms of the proportion of the
# population that is black and log of median income
log.modela <- lm(formula = log(psoda) ~ prpblck + lincome, data=data)
summary(log.modela)
# SRFa: log(est_psoda) = -0.7938 + 0.1216 prpblck + 0.0765 lincome + u
# Sample size: 410 fast-food resturants and their neighborhoods by zip code were measured.
# R-squared: 0.06809 - this means that only 6.81% of the variation of the predicted soda price is
# explained by the proportion of the population that is black AND the median houshold income.
# It is likely that other variables have a higher impact on the variation of the soda price, such
# as the costs of doing business in the various areas or the population density.
0.1216 * 0.2
# The predicted increase in soda price given the proportion of the population being black increases
# by 20% is approximately 2.43%
# B. Model examining the price of soda in its log form, in terms of only the variable that is
# proportion of the population that is black alone.
log.modelb <- lm(formula = log(psoda) ~ prpblck, data=data)
summary(log.modelb)
# SRFb: log(est_psoda) = 0.0331 + 0.0625 prpblck + u
# R-squared: 0.0183 - this means that only 1.83% of the variation of the predicted soda price is
# explained by the proportion of the population that is black alone. That is less than in the SRFa.
# The parameter associated with prpblck is lower in SRFb than SRFa. The fact that the value of this
# parameter changes when introducing new variables signifies that the two variables (prpblck and
# lincome) are correlated i.e. not independant. Although this breaks with a central assumptions
# (ZCM). Although we already know this is unlikely to be true with any real world data.
cor(data$lincome, data$prpblck, "na.or.complete")
cor(data$lincome, data$prppov, "na.or.complete")
load(file="ceosal1.RData")
desc
# A. In terms of the model parameters, state the null hypothesis that, after controlling for sales
# and roe, ros has no effect on CEO salary. State the alternative that better stock market
# performance increases a CEO's salary.
# H0: beta3_ros = 0
# We want to test at a statistical significance that a firm's return on stock does not
# affect the CEOs salary, after controlling for return on equity and log of sales.
# H1: beta3_ros != 0
# We reject that a firm's return on stock does not affect the CEOs salary at a statistical
# significance.
#
# Alternatively:
# H0: beta3_ros > 0
# We want to test at a statistical significance that a firm's return on stock does increases
# the CEOs salary.
# H1: beta3_ros =< 0
# We reject that a firm's return on stock affects the CEOs salary at a positively
# B Estimate the model log(salary) = β0 + β1 log(sales) + β2roe + β3ros + u
log.model <- lm(formula = salary ~ lsales + roe + ros, data = data)
summary(log.model)
# SRF: pred_salary = -1,491 + 287.1 lsales + 22.60 roe + 0.041 ros
# Sample size: 209 companies were survyed
# R-squared: 0.0572 - meaning that 5.72% of the variation in CEOs' salary could be predicted when
# the value of the variables lsales (log of sales), roe (return on equity), ros (return on stocks)
# are known.
0.041 * 0.05
# If a firm's return on stock were to increase by 50 basis points the predicted salary of its CEO
# would increase by 2.05 USD.
# C.Test the null hypothesis that ros has no effect on salary against the alternative that ros
# has a positive effect. Carry out the test at the 10% significance level (show the t-statistic
# and critical value used).
# alpha = 10%
# df = 209-3-1 = 205 -> thus the t-distribution approaches the standard normal distribution.
# For:
# H0: beta3_ros = 0
# H1: beta3_ros != 0
# t-statistic = 13.10
# critical value = 1.65
abs(qt(0.10/2, 205))
t.test(data$ros, data$ceosal, alternative = c("two.sided"))
# Since t-statistic > than the critical value we reject the null hypothesis that a firm's return
# on stocks does not affect the CEO's salary in our sample data.
#
# Alternatively:
# H0: beta3_ros > 0
# H1: beta3_ros =< 0
# t-statistic = 13.10
# critical value = 1.29
abs(qt(0.10, 205))
t.test(data$ros, data$ceosal, alternative = c("greater"))
# Since t-statistic > than the critical value we reject the null hypothesis that a firm's return
# on stocks does not affect the CEO's salary in our sample data.
# D. Should ros be included in a final model explaining CEO compensation?
# p-value in above tests = 2.2e-16 -> very low
# This P value indicates that if the return on stock had no effect, we would obtain the observed
# difference or more in 2.2e-14% of studies due to random sampling error in THIS DATA. So while
# it is unlikely that this data would assume a true null hypothesis, we can not with certainty
# rule out the possibility that our sample was unusual.
# Regardless, the t-statistic may prove there is some statistical value in the x3_ros, however,
# beta3_ros is relatively low and proves to have less economical or practical value. I would thus
# argue that it is not necessary to include this variable in a final model explaining the CEO
# compensation.
abs(qt(0.10, 205))
t.test(data$ros, data$ceosal, alternative = c("greater"))
log.model <- lm(formula = salary ~ lsales + roe + ros, data = data)
summary(log.model)
desc
log.model <- lm(formula = lsalary ~ lsales + roe + ros, data = data)
summary(log.model)
abs(qt(0.10, 205))
t.test(data$ros, data$ceosal, alternative = c("greater"))
View(data)
t.test(data$ros, data$lsalary, alternative = c("greater"))
t.test(data$ros, alternative = c("greater"))
log.model <- lm(formula = lsalary ~ lsales + roe + ros, data = data)
summary(log.model)
0.041 * 0.05
0.041 * 0.0002
0.0002 * 0.05
abs(qt(0.10, 205))
t.test(data$ros, alternative = c("greater"))
t.test(data$ros, alternative = c("greater"), conf.level = 0.9)
t.test(data$ros, data$lsalary, alternative = c("greater"), conf.level = 0.9)
log.model <- lm(formula = lsalary ~ lsales + roe + ros, data = data)
summary(log.model)
ttest[['statistic']]
t.test[['statistic']]
ttest[['statistic']]
ttest <- t.test(data$ros, data$lsalary, alternative = c("greater"), conf.level = 0.9)
ttest[['statistic']]
ttest <- t.test(data$ros, data$lsalary, alternative = c("greater"), conf.level = 0.9)
t.test(data$ros, data$lsalary, alternative = c("greater"), conf.level = 0.9)
log.model <- lm(formula = lsalary ~ lsales + roe + ros, data = data)
summary(log.model)
0.0002417/0.0005418
abs(qt(0.10, 205))
log.model <- lm(formula = lsalary ~ lsales + roe + ros, data = data)
summary(log.model)
abs(qt(0.05, 205))
